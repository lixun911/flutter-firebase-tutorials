{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lixun911/flutter-firebase-tutorials/blob/master/whole_system_even_git_server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, this is a much more comprehensive and self-contained system\\! Setting up your own Git server (like Gitea) within Docker alongside ArangoDB and your ingestion pipeline creates a fully isolated and reproducible environment for your AI coder project.\n",
        "\n",
        "Here's a step-by-step guide to implement and deploy the *whole system*, including a Git server, using Docker Compose:\n",
        "\n",
        "## System Architecture Overview\n",
        "\n",
        "```\n",
        "+-------------------+       +-------------------+       +-------------------+\n",
        "|                   |       |                   |       |                   |\n",
        "|   Git Server      | <---> |  Ingestion        | ----> |   ArangoDB        |\n",
        "|   (e.g., Gitea)   |       |  Pipeline         |       |   Knowledge Graph |\n",
        "|   (Container 1)   |       |  (Container 2)    |       |   (Container 3)   |\n",
        "|                   |       |                   |       |                   |\n",
        "+-------------------+       +-------------------+       +-------------------+\n",
        "      ^                                                         ^\n",
        "      |                                                         |\n",
        "      | (Host Volume for persistent Git data)                   | (Host Volume for persistent DB data)\n",
        "      |                                                         |\n",
        "      +---------------------------------------------------------+\n",
        "                          Your Host Machine\n",
        "```\n",
        "\n",
        "  * **Git Server (Gitea):** Manages your Git repositories. Your developers (or you) will push code here.\n",
        "  * **Ingestion Pipeline:** A Python application that clones/pulls from Gitea, parses the code, extracts relationships, generates embeddings, and pushes this structured data into ArangoDB.\n",
        "  * **ArangoDB:** Stores your multi-model knowledge graph (documents, graph, vectors, full-text search).\n",
        "\n",
        "## File Structure\n",
        "\n",
        "```\n",
        "/your-ai-coder-system/\n",
        "├── docker-compose.yml\n",
        "├── .env\n",
        "├── gitea_data/             # Persistent volume for Gitea data\n",
        "│   ├── gitea/\n",
        "│   └── git/\n",
        "├── arangodb_data/          # Persistent volume for ArangoDB data\n",
        "└── ingestion_pipeline/\n",
        "    ├── Dockerfile\n",
        "    ├── requirements.txt\n",
        "    └── app/\n",
        "        ├── main.py\n",
        "        ├── git_parser.py\n",
        "        └── arangodb_manager.py\n",
        "```\n",
        "\n",
        "## Step 1: Define Services in `docker-compose.yml`\n",
        "\n",
        "This file will orchestrate all three containers and their networking.\n",
        "\n",
        "```yaml\n",
        "# your-ai-coder-system/docker-compose.yml\n",
        "version: '3.8'\n",
        "\n",
        "networks:\n",
        "  ai_coder_net:\n",
        "    driver: bridge\n",
        "\n",
        "volumes:\n",
        "  gitea_data:\n",
        "  arangodb_data:\n",
        "\n",
        "services:\n",
        "  # 1. Gitea - Your Git Server\n",
        "  gitea:\n",
        "    image: gitea/gitea:1.21.11 # Using a stable version\n",
        "    container_name: gitea-server\n",
        "    environment:\n",
        "      - GITEA__DATABASE__TYPE=sqlite3 # Simple for local setup\n",
        "      - GITEA__DATABASE__PATH=/data/gitea/gitea.db\n",
        "      - GITEA__APP_NAME=Your AI Coder Git\n",
        "      - GITEA__SERVER__DOMAIN=localhost\n",
        "      - GITEA__SERVER__HTTP_PORT=3000\n",
        "      - GITEA__SERVER__ROOT_URL=http://localhost:3000/\n",
        "      - GITEA__SERVER__SSH_PORT=2222 # If you plan to use SSH for Git\n",
        "      - GITEA__SSH_GENERATED_KEY_TYPE=ed25519\n",
        "      - GITEA__SECURITY__INSTALL_LOCK=true # Prevents re-installation via web UI\n",
        "      - GITEA__ADMIN_USER=${GITEA_ADMIN_USER} # From .env\n",
        "      - GITEA__ADMIN_PASSWORD=${GITEA_ADMIN_PASSWORD} # From .env\n",
        "    volumes:\n",
        "      - gitea_data:/data # Persistent volume for Gitea's internal data and Git repos\n",
        "    ports:\n",
        "      - \"3000:3000\" # Web UI\n",
        "      - \"2222:2222\" # SSH for Git (optional, if you use SSH cloning)\n",
        "    networks:\n",
        "      - ai_coder_net\n",
        "    restart: always\n",
        "    healthcheck:\n",
        "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000/\"]\n",
        "      interval: 10s\n",
        "      timeout: 5s\n",
        "      retries: 5\n",
        "\n",
        "  # 2. ArangoDB - Your Knowledge Graph Database\n",
        "  arangodb:\n",
        "    image: arangodb/arangodb:latest\n",
        "    container_name: arangodb-instance\n",
        "    environment:\n",
        "      ARANGO_RANDOM_ROOT_PASSWORD: \"1\" # Generates a random root password\n",
        "      # ARANGO_ROOT_PASSWORD: \"${ARANGO_ROOT_PASSWORD}\" # Uncomment if you want a fixed password from .env\n",
        "    volumes:\n",
        "      - arangodb_data:/var/lib/arangodb3 # Persistent data volume\n",
        "    ports:\n",
        "      - \"8529:8529\" # ArangoDB Web UI and API\n",
        "    networks:\n",
        "      - ai_coder_net\n",
        "    restart: always\n",
        "    healthcheck:\n",
        "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8529/_admin/server/version\"]\n",
        "      interval: 10s\n",
        "      timeout: 5s\n",
        "      retries: 5\n",
        "\n",
        "  # 3. Ingestion Pipeline - Processes Git data into ArangoDB\n",
        "  ingestion_pipeline:\n",
        "    build: ./ingestion_pipeline # Build from Dockerfile in this directory\n",
        "    container_name: git-arangodb-ingest\n",
        "    depends_on:\n",
        "      gitea:\n",
        "        condition: service_healthy # Ensure Gitea is ready\n",
        "      arangodb:\n",
        "        condition: service_healthy # Ensure ArangoDB is ready\n",
        "    environment:\n",
        "      # ArangoDB Connection\n",
        "      ARANGO_HOST: arangodb-instance # Service name in Docker Compose network\n",
        "      ARANGO_PORT: 8529\n",
        "      ARANGO_USER: root\n",
        "      # For ARANGO_PASSWORD, you'll need to get it from ArangoDB logs if using random password,\n",
        "      # or set it in .env and pass it here if using a fixed password.\n",
        "      # For simplicity in this local setup with random password, we'll fetch it from logs manually for initial run.\n",
        "      # In a real system, use Docker Secrets or a dedicated secret management solution.\n",
        "      # ARANGO_PASSWORD: \"${ARANGO_ROOT_PASSWORD}\" # If using fixed password\n",
        "\n",
        "      # Gitea Connection (for cloning repos)\n",
        "      GITEA_HOST: gitea-server # Service name in Docker Compose network\n",
        "      GITEA_HTTP_PORT: 3000\n",
        "      GITEA_ADMIN_USER: ${GITEA_ADMIN_USER}\n",
        "      GITEA_ADMIN_PASSWORD: ${GITEA_ADMIN_PASSWORD}\n",
        "      # This example assumes you'll clone via HTTP with admin credentials.\n",
        "      # For production, consider using a dedicated Git user token or SSH keys.\n",
        "\n",
        "      # Git Repo to Ingest (example, you'll update this)\n",
        "      GIT_REPO_ORG: \"ai-coder-org\" # Example organization on Gitea\n",
        "      GIT_REPO_NAME: \"sample-codebase\" # Example repository name on Gitea\n",
        "      # The full URL will be constructed in the Python script\n",
        "    volumes:\n",
        "      - ./repos:/app/repos # Mount a volume for cloning Git repos\n",
        "    networks:\n",
        "      - ai_coder_net\n",
        "    # command: python /app/main.py # Uncomment if you want to explicitly define the entrypoint\n",
        "    # entrypoint: [\"python\", \"/app/main.py\"] # Alternative to CMD in Dockerfile\n",
        "```\n",
        "\n",
        "## Step 2: Create `.env` File\n",
        "\n",
        "This file will store sensitive information for Gitea.\n",
        "\n",
        "```\n",
        "# your-ai-coder-system/.env\n",
        "GITEA_ADMIN_USER=aicoder_admin\n",
        "GITEA_ADMIN_PASSWORD=SuperSecureGiteaPassw0rd!\n",
        "\n",
        "# If you want a fixed ArangoDB password instead of random\n",
        "# ARANGO_ROOT_PASSWORD=MySecureArangoDBPassw0rd!\n",
        "```\n",
        "\n",
        "## Step 3: Prepare Ingestion Pipeline Code\n",
        "\n",
        "The Python code for your ingestion pipeline (as detailed in the previous response) will go into the `ingestion_pipeline/` directory.\n",
        "\n",
        "### `ingestion_pipeline/Dockerfile` (No changes from previous response)\n",
        "\n",
        "```dockerfile\n",
        "# ingestion_pipeline/Dockerfile\n",
        "FROM python:3.10-slim-bookworm\n",
        "\n",
        "# Install Git and any other system dependencies needed for parsing/embeddings\n",
        "RUN apt-get update && \\\n",
        "    apt-get install -y git libmagic1 && \\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy requirements file first to leverage Docker cache\n",
        "COPY ingestion_pipeline/requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the application code\n",
        "COPY ingestion_pipeline/app/ .\n",
        "\n",
        "# Define a default command to run when the container starts\n",
        "CMD [\"python\", \"main.py\"]\n",
        "```\n",
        "\n",
        "### `ingestion_pipeline/requirements.txt` (No changes from previous response)\n",
        "\n",
        "```\n",
        "# ingestion_pipeline/requirements.txt\n",
        "python-arango\n",
        "GitPython\n",
        "tree_sitter\n",
        "tree_sitter_languages\n",
        "sentence-transformers # For embeddings\n",
        "requests\n",
        "```\n",
        "\n",
        "### `ingestion_pipeline/app/arangodb_manager.py` (Minor change for password handling)"
      ],
      "metadata": {
        "id": "hhR04DtiO8DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ingestion_pipeline/app/arangodb_manager.py\n",
        "from arango import ArangoClient\n",
        "import os\n",
        "\n",
        "class ArangoDBManager:\n",
        "    def __init__(self, arango_password=None): # Added arango_password parameter\n",
        "        self.client = ArangoClient(hosts=f\"http://{os.getenv('ARANGO_HOST')}:{os.getenv('ARANGO_PORT')}\")\n",
        "        self.db = None\n",
        "        # Use the password passed during initialization or from environment variable\n",
        "        self.arango_password = arango_password if arango_password else os.getenv('ARANGO_PASSWORD')\n",
        "        self.connect()\n",
        "\n",
        "    def connect(self):\n",
        "        try:\n",
        "            self.db = self.client.db(\n",
        "                \"_system\", # Or your specific database name\n",
        "                username=os.getenv('ARANGO_USER'),\n",
        "                password=self.arango_password # Use the stored password\n",
        "            )\n",
        "            print(\"Connected to ArangoDB successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error connecting to ArangoDB: {e}\")\n",
        "            raise\n",
        "\n",
        "    # ... rest of the class methods (create_collections_and_graphs, insert_document, insert_edge, execute_aql)\n",
        "    # ... are the same as in the previous response.\n",
        "    # For brevity, I'm omitting them here, but ensure they are included in your actual file.\n",
        "\n",
        "    def create_collections_and_graphs(self):\n",
        "        # Vertex collections\n",
        "        for col_name in [\"Commits\", \"Developers\", \"Files\", \"Classes\", \"Methods\", \"Features\", \"Bugs\"]:\n",
        "            if not self.db.has_collection(col_name):\n",
        "                self.db.create_collection(col_name)\n",
        "                print(f\"Created collection: {col_name}\")\n",
        "\n",
        "        # Edge collections (for relationships)\n",
        "        for edge_name in [\"AuthoredBy\", \"Modifies\", \"Calls\", \"Imports\", \"DependsOn\", \"Fixes\", \"Implements\"]:\n",
        "            if not self.db.has_collection(edge_name):\n",
        "                self.db.create_collection(edge_name, edge=True)\n",
        "                print(f\"Created edge collection: {edge_name}\")\n",
        "\n",
        "        # Create a named graph (optional, but good for logical grouping)\n",
        "        if not self.db.has_graph(\"SoftwareKnowledgeGraph\"):\n",
        "            graph = self.db.create_graph(\"SoftwareKnowledgeGraph\")\n",
        "            graph.create_edge_definition(\n",
        "                edge_collection=\"AuthoredBy\",\n",
        "                from_vertex_collections=[\"Commits\"],\n",
        "                to_vertex_collections=[\"Developers\"]\n",
        "            )\n",
        "            graph.create_edge_definition(\n",
        "                edge_collection=\"Modifies\",\n",
        "                from_vertex_collections=[\"Commits\"],\n",
        "                to_vertex_collections=[\"Files\", \"Classes\", \"Methods\"]\n",
        "            )\n",
        "            graph.create_edge_definition(\n",
        "                edge_collection=\"Calls\",\n",
        "                from_vertex_collections=[\"Methods\"],\n",
        "                to_vertex_collections=[\"Methods\", \"Classes\"] # Methods call methods/classes\n",
        "            )\n",
        "            # ... add other edge definitions\n",
        "            print(\"Created graph: SoftwareKnowledgeGraph\")\n",
        "\n",
        "    def insert_document(self, collection_name, data, key=None):\n",
        "        try:\n",
        "            collection = self.db.collection(collection_name)\n",
        "            if key:\n",
        "                data['_key'] = key\n",
        "            doc = collection.insert(data, overwrite=True) # overwrite on re-ingestion\n",
        "            return doc['_id']\n",
        "        except Exception as e:\n",
        "            print(f\"Error inserting document into {collection_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def insert_edge(self, edge_collection_name, from_vertex_id, to_vertex_id, properties=None):\n",
        "        try:\n",
        "            edge_collection = self.db.collection(edge_collection_name)\n",
        "            edge_data = {\"_from\": from_vertex_id, \"_to\": to_vertex_id}\n",
        "            if properties:\n",
        "                edge_data.update(properties)\n",
        "            edge = edge_collection.insert(edge_data, overwrite=True)\n",
        "            return edge['_id']\n",
        "        except Exception as e:\n",
        "            print(f\"Error inserting edge into {edge_collection_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def execute_aql(self, query, bind_vars=None):\n",
        "        try:\n",
        "            cursor = self.db.aql.execute(query, bind_vars=bind_vars)\n",
        "            return list(cursor)\n",
        "        except Exception as e:\n",
        "            print(f\"Error executing AQL query: {e}\")\n",
        "            raise"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "olljWiSMO8Db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `ingestion_pipeline/app/git_parser.py` (Updated for Gitea Auth)"
      ],
      "metadata": {
        "id": "0MIcoGljO8Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ingestion_pipeline/app/git_parser.py\n",
        "import git\n",
        "import os\n",
        "from tree_sitter import Language, Parser\n",
        "from tree_sitter_languages import get_language, get_parser\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import hashlib # Import hashlib for better key generation\n",
        "import urllib.parse # Import urllib.parse for URL encoding\n",
        "\n",
        "# Load embedding model (consider a specific code-embedding model if needed)\n",
        "try:\n",
        "    embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "except Exception:\n",
        "    print(\"Could not load SentenceTransformer model. Embeddings will be skipped.\")\n",
        "    embedding_model = None\n",
        "\n",
        "\n",
        "class GitRepoParser:\n",
        "    def __init__(self, repo_url, local_path, git_username=None, git_password=None):\n",
        "        self.repo_url = repo_url\n",
        "        self.local_path = local_path\n",
        "        self.git_username = git_username\n",
        "        self.git_password = git_password\n",
        "        self.repo = self._clone_or_pull()\n",
        "        self.parsers = {} # For Tree-sitter parsers\n",
        "\n",
        "    def _clone_or_pull(self):\n",
        "        # Construct authenticated URL with URL encoding for credentials\n",
        "        auth_repo_url = self.repo_url\n",
        "        if self.git_username and self.git_password:\n",
        "            # Assumes HTTP/HTTPS cloning\n",
        "            protocol, rest = self.repo_url.split('://', 1)\n",
        "            encoded_username = urllib.parse.quote_plus(self.git_username)\n",
        "            encoded_password = urllib.parse.quote_plus(self.git_password)\n",
        "            auth_repo_url = f\"{protocol}://{encoded_username}:{encoded_password}@{rest}\"\n",
        "\n",
        "        if os.path.exists(self.local_path):\n",
        "            print(f\"Pulling latest from {self.repo_url} into {self.local_path}...\")\n",
        "            repo = git.Repo(self.local_path)\n",
        "            origin = repo.remotes.origin\n",
        "            # Use authenticated URL for pull\n",
        "            try:\n",
        "                origin.pull(url=auth_repo_url) # Explicitly pass authenticated URL for pull\n",
        "            except git.exc.GitCommandError as e:\n",
        "                print(f\"Error pulling from origin: {e}\")\n",
        "                print(\"Attempting to re-clone...\")\n",
        "                # If pull fails, try re-cloning (might be due to auth or repo state)\n",
        "                if os.path.exists(self.local_path):\n",
        "                    try:\n",
        "                        import shutil\n",
        "                        shutil.rmtree(self.local_path)\n",
        "                    except OSError as e:\n",
        "                        print(f\"Error removing directory {self.local_path}: {e}\")\n",
        "                        raise # Re-raise if directory removal fails\n",
        "                return git.Repo.clone_from(auth_repo_url, self.local_path)\n",
        "            return repo\n",
        "        else:\n",
        "            print(f\"Cloning {self.repo_url} into {self.local_path}...\")\n",
        "            # Use authenticated URL for cloning\n",
        "            return git.Repo.clone_from(auth_repo_url, self.local_path)\n",
        "\n",
        "    def _get_tree_sitter_parser(self, language_name):\n",
        "        if language_name not in self.parsers:\n",
        "            try:\n",
        "                lang = get_language(language_name)\n",
        "                parser = Parser()\n",
        "                parser.set_language(lang)\n",
        "                self.parsers[language_name] = parser\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not load Tree-sitter parser for {language_name}: {e}\")\n",
        "                self.parsers[language_name] = None\n",
        "        return self.parsers[language_name]\n",
        "\n",
        "    def _parse_code_structure(self, file_content, file_extension):\n",
        "        # Basic mapping from extension to Tree-sitter language\n",
        "        lang_map = {\n",
        "            '.py': 'python',\n",
        "            '.js': 'javascript',\n",
        "            '.java': 'java',\n",
        "            '.go': 'go',\n",
        "            '.ts': 'typescript',\n",
        "            '.c': 'c',\n",
        "            '.cpp': 'cpp',\n",
        "            '.h': 'c',\n",
        "            '.hpp': 'cpp',\n",
        "            '.rb': 'ruby',\n",
        "            '.php': 'php',\n",
        "            '.sh': 'bash',\n",
        "            '.rs': 'rust',\n",
        "            '.swift': 'swift',\n",
        "            '.kt': 'kotlin',\n",
        "            '.scala': 'scala',\n",
        "            '.r': 'r',\n",
        "            '.m': 'matlab', # Or objective-c\n",
        "            '.json': 'json',\n",
        "            '.yaml': 'yaml',\n",
        "            '.yml': 'yaml',\n",
        "            '.xml': 'xml',\n",
        "            '.html': 'html',\n",
        "            '.css': 'css',\n",
        "            '.sql': 'sql',\n",
        "            '.pl': 'perl',\n",
        "            '.lua': 'lua',\n",
        "            '.groovy': 'groovy',\n",
        "            # Removed potentially unavailable parsers: 'assembly', 'batch', 'powershell'\n",
        "        }\n",
        "        language_name = lang_map.get(file_extension.lower())\n",
        "        if not language_name:\n",
        "            print(f\"No Tree-sitter mapping for extension: {file_extension}\")\n",
        "            return [] # No parser for this language\n",
        "\n",
        "        parser = self._get_tree_sitter_parser(language_name)\n",
        "        if not parser:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            tree = parser.parse(bytes(file_content, \"utf8\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing file with tree-sitter ({language_name}): {e}\")\n",
        "            return []\n",
        "\n",
        "        methods_classes = []\n",
        "\n",
        "        def traverse(node):\n",
        "            try:\n",
        "                if node.type in ['function_definition', 'method_definition', 'class_definition', 'struct_definition', 'enum_definition']:\n",
        "                     # Attempt to find a common pattern for name (often an 'identifier' or similar)\n",
        "                     name_node = None\n",
        "                     for child in node.children:\n",
        "                          if child.type in ['identifier', 'type_identifier', 'name', 'declaration_identifier']:\n",
        "                              name_node = child\n",
        "                              break\n",
        "                          # Handle Python specific decorated_definition\n",
        "                          if language_name == 'python' and child.type == 'decorated_definition':\n",
        "                              for sub_child in child.children:\n",
        "                                  if sub_child.type in ['function_definition', 'class_definition']:\n",
        "                                       for identifier_child in sub_child.children:\n",
        "                                            if identifier_child.type == 'identifier':\n",
        "                                                 name_node = identifier_child\n",
        "                                                 break\n",
        "                                       if name_node:\n",
        "                                            break\n",
        "                              if name_node:\n",
        "                                 break\n",
        "\n",
        "                     name = name_node.text.decode('utf8') if name_node and name_node.text else 'unknown'\n",
        "\n",
        "                     # Basic check to avoid parsing issues with incomplete nodes\n",
        "                     if node.start_byte >= node.end_byte:\n",
        "                          return\n",
        "\n",
        "                     methods_classes.append({\n",
        "                         \"type\": node.type,\n",
        "                         \"name\": name,\n",
        "                         \"start_line\": node.start_point[0] + 1,\n",
        "                         \"end_line\": node.end_point[0] + 1,\n",
        "                         \"code_snippet\": file_content[node.start_byte:node.end_byte],\n",
        "                     })\n",
        "\n",
        "                for child in node.children:\n",
        "                    traverse(child)\n",
        "            except Exception as e:\n",
        "                # Catch exceptions during traversal to avoid stopping the whole process\n",
        "                print(f\"Warning: Error during Tree-sitter traversal in {language_name} for node type {node.type}: {e}\")\n",
        "\n",
        "\n",
        "        traverse(tree.root_node)\n",
        "        return methods_classes\n",
        "\n",
        "\n",
        "    def _generate_embedding(self, text):\n",
        "        if embedding_model:\n",
        "            try:\n",
        "                # Add a check for text length or size if memory is a concern\n",
        "                if len(text) > 10000: # Example threshold\n",
        "                    print(f\"Warning: Skipping embedding for large text snippet ({len(text)} characters).\")\n",
        "                    return None\n",
        "                return embedding_model.encode(text).tolist()\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating embedding: {e}\")\n",
        "                return None\n",
        "        return None # Return None if no model is loaded\n",
        "\n",
        "    def get_repo_data(self):\n",
        "        commits_data = []\n",
        "        try:\n",
        "            # Fetch more commits if needed, or implement incremental logic\n",
        "            for commit in self.repo.iter_commits('main', max_count=100): # Increased limit for testing\n",
        "                commit_info = {\n",
        "                    \"hash\": commit.hexsha,\n",
        "                    \"author_name\": commit.author.name,\n",
        "                    \"author_email\": commit.author.email,\n",
        "                    \"committer_name\": commit.committer.name,\n",
        "                    \"committer_email\": commit.committer.email,\n",
        "                    \"timestamp\": commit.authored_datetime.isoformat(),\n",
        "                    \"message\": commit.message,\n",
        "                    \"lines_added\": 0,\n",
        "                    \"lines_deleted\": 0,\n",
        "                    \"files_changed\": []\n",
        "                }\n",
        "\n",
        "                files_modified = []\n",
        "                methods_classes_modified = []\n",
        "\n",
        "                if commit.parents:\n",
        "                    diff_index = commit.diff(commit.parents[0], create_patch=True)\n",
        "                else:\n",
        "                    diff_index = commit.diff(None, create_patch=True)\n",
        "\n",
        "                for diff in diff_index:\n",
        "                    file_path = diff.b_path if diff.b_path else diff.a_path\n",
        "                    if file_path: # Ensure file_path is not None\n",
        "                         commit_info[\"files_changed\"].append(file_path)\n",
        "\n",
        "                    try:\n",
        "                        if diff.change_type == 'A':\n",
        "                            commit_info[\"lines_added\"] += diff.diff.decode(errors='ignore').count('\\n+')\n",
        "                        elif diff.change_type == 'D':\n",
        "                            commit_info[\"lines_deleted\"] += diff.diff.decode(errors='ignore').count('\\n-')\n",
        "                        elif diff.change_type == 'M':\n",
        "                            commit_info[\"lines_added\"] += diff.diff.decode(errors='ignore').count('\\n+')\n",
        "                            commit_info[\"lines_deleted\"] += diff.diff.decode(errors='ignore').count('\\n-')\n",
        "                    except Exception as e:\n",
        "                         print(f\"Warning: Could not decode diff for {file_path} in commit {commit.hexsha}: {e}\")\n",
        "\n",
        "\n",
        "                    if diff.change_type in ['A', 'M'] and diff.b_path: # Process only if file exists after change\n",
        "                        try:\n",
        "                            blob = commit.tree / diff.b_path\n",
        "                            file_content = blob.data_stream.read().decode('utf-8', errors='ignore')\n",
        "                            _, file_extension = os.path.splitext(file_path)\n",
        "\n",
        "                            parsed_elements = self._parse_code_structure(file_content, file_extension)\n",
        "                            for el in parsed_elements:\n",
        "                                el['file_path'] = file_path\n",
        "                                el['embedding'] = self._generate_embedding(el['code_snippet'])\n",
        "                                methods_classes_modified.append(el)\n",
        "\n",
        "                            files_modified.append({\n",
        "                                \"path\": file_path,\n",
        "                                \"content\": file_content,\n",
        "                                \"content_hash\": hashlib.sha256(file_content.encode('utf-8', errors='ignore')).hexdigest(), # Add content hash\n",
        "                                \"file_embedding\": self._generate_embedding(file_content),\n",
        "                                \"parsed_elements\": parsed_elements\n",
        "                            })\n",
        "                        except Exception as e:\n",
        "                            print(f\"Warning: Could not read or parse file {file_path} in commit {commit.hexsha}: {e}\")\n",
        "                    elif diff.change_type == 'D' and diff.a_path:\n",
        "                         # Handle deleted files - you might want to mark them as deleted in ArangoDB\n",
        "                         print(f\"File deleted: {diff.a_path} in commit {commit.hexsha}\")\n",
        "                         # TODO: Implement logic to mark file as deleted in ArangoDB\n",
        "\n",
        "                commit_info[\"files_detailed\"] = files_modified\n",
        "                commit_info[\"parsed_elements_modified\"] = methods_classes_modified\n",
        "                commits_data.append(commit_info)\n",
        "        except Exception as e:\n",
        "             print(f\"Error fetching repository data: {e}\")\n",
        "             # Continue with any commits fetched so far, or raise the exception\n",
        "             # raise # Uncomment to stop on error\n",
        "\n",
        "        return commits_data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "5l-2q8a7O8Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `ingestion_pipeline/app/main.py` (Updated to use Gitea credentials and URL)"
      ],
      "metadata": {
        "id": "luDMMAu0O8Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ingestion_pipeline/app/main.py\n",
        "import os\n",
        "import time\n",
        "from arangodb_manager import ArangoDBManager\n",
        "from git_parser import GitRepoParser\n",
        "import hashlib # Import hashlib for file key generation\n",
        "import re # Import re for key validation\n",
        "\n",
        "def run_ingestion():\n",
        "    # --- 1. Get ArangoDB Password ---\n",
        "    # Use the fixed password from .env set in docker-compose.yml\n",
        "    arangodb_password = os.getenv('ARANGO_PASSWORD') # This should now be set from .env\n",
        "\n",
        "    if not arangodb_password:\n",
        "        print(\"\\n--- ATTENTION: ArangoDB password not set. ---\")\n",
        "        print(\"Please ensure ARANGO_ROOT_PASSWORD is set in your .env file and passed to the ingestion_pipeline service in docker-compose.yml.\")\n",
        "        print(\"Ingestion cannot proceed without the ArangoDB password.\")\n",
        "        return # Exit if DB connection fails\n",
        "\n",
        "    try:\n",
        "        arangodb_manager = ArangoDBManager(arangodb_password=arangodb_password)\n",
        "        arangodb_manager.create_collections_and_graphs()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to connect to ArangoDB: {e}\")\n",
        "        print(\"Ensure ArangoDB is running and the password is correct.\")\n",
        "        return # Exit if DB connection fails\n",
        "\n",
        "\n",
        "    # --- 2. Prepare Git Repo URL with Gitea credentials ---\n",
        "    gitea_host = os.getenv('GITEA_HOST')\n",
        "    gitea_http_port = os.getenv('GITEA_HTTP_PORT')\n",
        "    gitea_admin_user = os.getenv('GITEA_ADMIN_USER')\n",
        "    gitea_admin_password = os.getenv('GITEA_ADMIN_PASSWORD')\n",
        "    git_repo_org = os.getenv('GIT_REPO_ORG')\n",
        "    git_repo_name = os.getenv('GIT_REPO_NAME')\n",
        "\n",
        "    # Construct the authenticated Git URL\n",
        "    # This assumes HTTP/HTTPS cloning. For SSH, it's more complex (SSH agent forwarding/keys).\n",
        "    # Ensure all necessary environment variables are set\n",
        "    if not all([gitea_host, gitea_http_port, gitea_admin_user, gitea_admin_password, git_repo_org, git_repo_name]):\n",
        "         print(\"Missing one or more Gitea environment variables. Cannot proceed with Git cloning.\")\n",
        "         return\n",
        "\n",
        "    # The GitRepoParser will handle URL encoding of credentials\n",
        "    repo_url = f\"http://{gitea_host}:{gitea_http_port}/{git_repo_org}/{git_repo_name}.git\"\n",
        "    local_repo_path = f\"/app/repos/{git_repo_name}\" # Path inside the Docker container\n",
        "\n",
        "    try:\n",
        "        parser = GitRepoParser(repo_url, local_repo_path, gitea_admin_user, gitea_admin_password)\n",
        "        all_commits_data = parser.get_repo_data()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to clone or parse Git repository: {e}\")\n",
        "        return # Exit if Git operations fail\n",
        "\n",
        "\n",
        "    print(f\"Processing {len(all_commits_data)} commits...\")\n",
        "\n",
        "    # Helper function to validate ArangoDB keys\n",
        "    def is_valid_arangodb_key(key):\n",
        "        # ArangoDB keys must be strings and can only contain the characters a-z, A-Z, 0-9,\n",
        "        # as well as the dash (-), underscore (_), colon (:), dot (.), and at sign (@).\n",
        "        # Keys cannot start with a dash or dot.\n",
        "        if not isinstance(key, str) or not key:\n",
        "            return False\n",
        "        if key[0] in ['-', '.']:\n",
        "            return False\n",
        "        return re.fullmatch(r'[a-zA-Z0-9\\-_:@.]+', key) is not None\n",
        "\n",
        "\n",
        "    for commit_data in all_commits_data:\n",
        "        try:\n",
        "            # 1. Ingest Developer\n",
        "            # Using email hash for a more robust key and validating it\n",
        "            developer_key = hashlib.sha256(commit_data[\"author_email\"].encode()).hexdigest()\n",
        "            if not is_valid_arangodb_key(developer_key):\n",
        "                print(f\"Warning: Generated invalid ArangoDB key for developer email {commit_data['author_email']}. Skipping.\")\n",
        "                developer_id = None\n",
        "            else:\n",
        "                developer_id = arangodb_manager.insert_document(\n",
        "                    \"Developers\",\n",
        "                    {\"name\": commit_data[\"author_name\"], \"email\": commit_data[\"author_email\"]},\n",
        "                    key=developer_key\n",
        "                )\n",
        "\n",
        "            # 2. Ingest Commit\n",
        "            # Using commit hash as key and validating it\n",
        "            commit_key = commit_data[\"hash\"]\n",
        "            if not is_valid_arangodb_key(commit_key):\n",
        "                 print(f\"Warning: Generated invalid ArangoDB key for commit hash {commit_data['hash']}. Skipping.\")\n",
        "                 commit_id = None\n",
        "            else:\n",
        "                commit_id = arangodb_manager.insert_document(\n",
        "                    \"Commits\",\n",
        "                    {\n",
        "                        \"hash\": commit_data[\"hash\"],\n",
        "                        \"message\": commit_data[\"message\"],\n",
        "                        \"timestamp\": commit_data[\"timestamp\"],\n",
        "                        \"lines_added\": commit_data[\"lines_added\"],\n",
        "                        \"lines_deleted\": commit_data[\"lines_deleted\"],\n",
        "                        \"author_email\": commit_data[\"author_email\"],\n",
        "                    },\n",
        "                    key=commit_key\n",
        "                )\n",
        "\n",
        "            # 3. Create AuthoredBy relationship\n",
        "            if commit_id and developer_id:\n",
        "                arangodb_manager.insert_edge(\"AuthoredBy\", commit_id, developer_id)\n",
        "\n",
        "            # 4. Ingest Files, Classes, Methods, and Modifies relationships\n",
        "            for file_data in commit_data[\"files_detailed\"]:\n",
        "                # Using path hash for a more robust key and validating it\n",
        "                file_key = hashlib.sha256(file_data[\"path\"].encode()).hexdigest()\n",
        "                if not is_valid_arangodb_key(file_key):\n",
        "                    print(f\"Warning: Generated invalid ArangoDB key for file path {file_data['path']}. Skipping file.\")\n",
        "                    file_id = None\n",
        "                else:\n",
        "                    file_id = arangodb_manager.insert_document(\n",
        "                        \"Files\",\n",
        "                        {\n",
        "                            \"path\": file_data[\"path\"],\n",
        "                            \"content_hash\": file_data[\"content_hash\"], # Use the hash generated in parser\n",
        "                            \"last_commit_hash\": commit_data[\"hash\"],\n",
        "                            \"embedding\": file_data[\"file_embedding\"]\n",
        "                        },\n",
        "                        key=file_key\n",
        "                    )\n",
        "                    if commit_id and file_id:\n",
        "                        arangodb_manager.insert_edge(\"Modifies\", commit_id, file_id)\n",
        "\n",
        "                    for element in file_data[\"parsed_elements\"]:\n",
        "                        # Using a composite key including file hash, element type, name, and start line and validating it\n",
        "                        element_composite_key_str = f\"{file_key}_{element['type']}_{element['name']}_{element['start_line']}\"\n",
        "                        element_key = hashlib.sha256(element_composite_key_str.encode()).hexdigest()\n",
        "\n",
        "                        if not is_valid_arangodb_key(element_key):\n",
        "                             print(f\"Warning: Generated invalid ArangoDB key for element in file {file_data['path']}. Skipping element.\")\n",
        "                             continue # Skip this element\n",
        "\n",
        "                        if element[\"type\"] in [\"class_definition\", \"struct_definition\", \"enum_definition\"]:\n",
        "                            class_id = arangodb_manager.insert_document(\n",
        "                                \"Classes\", # Assuming Classes collection can hold various code structure types\n",
        "                                {\n",
        "                                    \"name\": element[\"name\"],\n",
        "                                    \"type\": element[\"type\"], # Store the specific type\n",
        "                                    \"file_path\": element[\"file_path\"],\n",
        "                                    \"start_line\": element[\"start_line\"],\n",
        "                                    \"end_line\": element[\"end_line\"],\n",
        "                                    \"code_snippet\": element[\"code_snippet\"],\n",
        "                                    \"embedding\": element[\"embedding\"]\n",
        "                                },\n",
        "                                key=element_key\n",
        "                            )\n",
        "                            if commit_id and class_id:\n",
        "                                arangodb_manager.insert_edge(\"Modifies\", commit_id, class_id)\n",
        "                        elif element[\"type\"] in [\"function_definition\", \"method_definition\"]:\n",
        "                            method_id = arangodb_manager.insert_document(\n",
        "                                \"Methods\",\n",
        "                                {\n",
        "                                    \"name\": element[\"name\"],\n",
        "                                    \"type\": element[\"type\"], # Store the specific type\n",
        "                                    \"file_path\": element[\"file_path\"],\n",
        "                                    \"start_line\": element[\"start_line\"],\n",
        "                                    \"end_line\": element[\"end_line\"],\n",
        "                                    \"code_snippet\": element[\"code_snippet\"],\n",
        "                                    \"embedding\": element[\"embedding\"]\n",
        "                                },\n",
        "                                key=element_key\n",
        "                            )\n",
        "                            if commit_id and method_id:\n",
        "                                arangodb_manager.insert_edge(\"Modifies\", commit_id, method_id)\n",
        "\n",
        "                # TODO: Implement parsing for 'Calls', 'Imports', 'DependsOn' relationships\n",
        "                # This often requires more sophisticated AST analysis across files/modules.\n",
        "\n",
        "                # TODO: Integrate with Jira/other external systems for 'Fixes', 'Implements' relationships\n",
        "                # This would involve querying Jira API using commit messages (e.g., regex for JIRA-XXX)\n",
        "                # and then ingesting 'Features' and 'Bugs' nodes and 'Fixes'/'Implements' edges.\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing commit {commit_data.get('hash', 'N/A')}: {e}\")\n",
        "            # Continue to next commit, or break if error is critical\n",
        "            # break # Uncomment to stop on first commit error\n",
        "\n",
        "\n",
        "    print(\"Ingestion complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_ingestion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "KB5bYCzbO8Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Deploy and Run the System\n",
        "\n",
        "1.  **Create Directories:**\n",
        "\n",
        "    ```bash\n",
        "    mkdir your-ai-coder-system\n",
        "    cd your-ai-coder-system\n",
        "    mkdir gitea_data arangodb_data repos ingestion_pipeline ingestion_pipeline/app\n",
        "    ```\n",
        "\n",
        "2.  **Place Files:**\n",
        "\n",
        "      * `docker-compose.yml` into `your-ai-coder-system/`\n",
        "      * `.env` into `your-ai-coder-system/`\n",
        "      * `Dockerfile` into `your-ai-coder-system/ingestion_pipeline/`\n",
        "      * `requirements.txt` into `your-ai-coder-system/ingestion_pipeline/`\n",
        "      * `main.py`, `git_parser.py`, `arangodb_manager.py` into `your-ai-coder-system/ingestion_pipeline/app/`\n",
        "\n",
        "3.  **Build and Start Services:**\n",
        "    Navigate to the `your-ai-coder-system/` directory in your terminal and run:\n",
        "\n",
        "    ```bash\n",
        "    docker compose up -d --build\n",
        "    ```\n",
        "\n",
        "      * `--build` ensures your `ingestion_pipeline` image is built.\n",
        "      * `-d` runs containers in detached mode.\n",
        "\n",
        "4.  **Initial Gitea Setup (Manual - First Time Only):**\n",
        "\n",
        "      * Open your browser to `http://localhost:3000`.\n",
        "      * Gitea should present an initial configuration page.\n",
        "      * **Crucially:** Since you set `GITEA__SECURITY__INSTALL_LOCK=true` and provided `GITEA_ADMIN_USER`/`GITEA_ADMIN_PASSWORD` in `.env`, Gitea should automatically set up the admin user and lock the installation. You might just see a login page.\n",
        "      * Log in with the `GITEA_ADMIN_USER` and `GITEA_ADMIN_PASSWORD` from your `.env` file.\n",
        "      * **Create an Organization:** Create a new organization, e.g., `ai-coder-org` (matching `GIT_REPO_ORG` in `docker-compose.yml`).\n",
        "      * **Create a Repository:** Inside that organization, create a new repository, e.g., `sample-codebase` (matching `GIT_REPO_NAME`). Initialize it with a README.\n",
        "      * **Push Sample Code:** Clone this empty repository to your *host machine* (e.g., `git clone http://localhost:3000/ai-coder-org/sample-codebase.git`). Add some sample Python, JavaScript, or Java files to it. Commit and push these files back to Gitea. This provides data for your ingestion pipeline.\n",
        "\n",
        "5.  **Run the Ingestion Pipeline:**\n",
        "    Once Gitea has some code, you can trigger the ingestion.\n",
        "\n",
        "    ```bash\n",
        "    docker compose up ingestion_pipeline\n",
        "    ```\n",
        "\n",
        "      * This will restart the `ingestion_pipeline` container. It will connect to Gitea, clone the `sample-codebase` repository, process its commits, and ingest the data into ArangoDB.\n",
        "      * Check logs: `docker logs git-arangodb-ingest` to see the progress and any errors.\n",
        "\n",
        "6.  **Verify Data in ArangoDB:**\n",
        "\n",
        "      * Open your browser to `http://localhost:8529`.\n",
        "      * Log in to ArangoDB (if using a random password, get it from `docker logs arangodb-instance`).\n",
        "      * Navigate to the \"Collections\" and \"Graphs\" sections to see the `Commits`, `Developers`, `Files`, `Classes`, `Methods` collections and the `SoftwareKnowledgeGraph`. You should see your ingested data\\!\n",
        "\n",
        "### Important Considerations and Next Steps:\n",
        "\n",
        "  * **ArangoDB Password Management:** For production, avoid `ARANGO_RANDOM_ROOT_PASSWORD=1`. Instead, set `ARANGO_ROOT_PASSWORD` in your `.env` and pass it to both `arangodb` and `ingestion_pipeline` services in `docker-compose.yml`. Even better, use Docker Secrets.\n",
        "  * **Gitea Authentication:** Using the Gitea admin password directly in the `GIT_REPO_URL` is convenient for a local demo but insecure for production. For production, create a dedicated Git user with a personal access token for the ingestion pipeline, and use that token in the URL.\n",
        "  * **Incremental Sync:** The `git_parser.py` currently fetches a limited number of recent commits. You need to implement the logic for incremental updates:\n",
        "    1.  Store the hash of the *last successfully processed commit* in ArangoDB.\n",
        "    2.  On subsequent runs, retrieve this hash.\n",
        "    3.  Call `parser.get_repo_data(since_commit_hash=last_hash)` to only process new commits.\n",
        "    4.  Update the `last_processed_commit_hash` in ArangoDB after a successful run.\n",
        "    5.  Handle file renames and deletions (Git diffs will show these; you'll need to update/delete corresponding nodes in ArangoDB).\n",
        "  * **AST Parsing Depth:** The `_parse_code_structure` in `git_parser.py` is a very basic example. Real-world code parsing for `Calls` and `Imports` relationships requires much more sophisticated AST traversal and symbol resolution. Libraries like `tree-sitter` are powerful but require careful implementation for deep code analysis.\n",
        "  * **External Integrations (Jira/Features/Bugs):**\n",
        "      * You'll need to add code to `main.py` to connect to your Jira (or other issue tracker) API.\n",
        "      * Parse commit messages for issue IDs (e.g., regex `[A-Z]+-\\d+`).\n",
        "      * Query the Jira API for details about those issues.\n",
        "      * Ingest `(:Feature)` and `(:Bug)` nodes and `(:Fixes)` / `(:Implements)` edges, linking them to `(:Commit)` nodes.\n",
        "  * **Scheduling:** Once the incremental sync is implemented, decide how to run it:\n",
        "      * A `cron` job on your host that runs `docker compose up ingestion_pipeline` periodically.\n",
        "      * A Git webhook from Gitea that triggers this Docker Compose command (requires a small web server to receive the webhook).\n",
        "  * **Resource Management:** Monitor CPU, memory, and disk usage, especially as your codebase and knowledge graph grow. Adjust Docker resource limits if necessary.\n",
        "\n",
        "This detailed setup provides a robust foundation for your AI coder's knowledge graph."
      ],
      "metadata": {
        "id": "itti9STSO8Dc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab50f2a7"
      },
      "source": [
        "Okay, this is a much more comprehensive and self-contained system\\! Setting up your own Git server (like Gitea) within Docker alongside ArangoDB and your ingestion pipeline creates a fully isolated and reproducible environment for your AI coder project.\n",
        "\n",
        "Here's a step-by-step guide to implement and deploy the *whole system*, including a Git server, using Docker Compose:\n",
        "\n",
        "## System Architecture Overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e508ef6"
      },
      "source": [
        "# your-ai-coder-system/docker-compose.yml\n",
        "version: '3.8'\n",
        "\n",
        "networks:\n",
        "  ai_coder_net:\n",
        "    driver: bridge\n",
        "\n",
        "volumes:\n",
        "  gitea_data:\n",
        "  arangodb_data:\n",
        "\n",
        "services:\n",
        "  # 1. Gitea - Your Git Server\n",
        "  gitea:\n",
        "    image: gitea/gitea:1.21.11 # Using a stable version\n",
        "    container_name: gitea-server\n",
        "    environment:\n",
        "      - GITEA__DATABASE__TYPE=sqlite3 # Simple for local setup\n",
        "      - GITEA__DATABASE__PATH=/data/gitea/gitea.db\n",
        "      - GITEA__APP_NAME=Your AI Coder Git\n",
        "      - GITEA__SERVER__DOMAIN=localhost\n",
        "      - GITEA__SERVER__HTTP_PORT=3000\n",
        "      - GITEA__SERVER__ROOT_URL=http://localhost:3000/\n",
        "      - GITEA__SERVER__SSH_PORT=2222 # If you plan to use SSH for Git\n",
        "      - GITEA__SSH_GENERATED_KEY_TYPE=ed25519\n",
        "      - GITEA__SECURITY__INSTALL_LOCK=true # Prevents re-installation via web UI\n",
        "      - GITEA__ADMIN_USER=${GITEA_ADMIN_USER} # From .env\n",
        "      - GITEA__ADMIN_PASSWORD=${GITEA_ADMIN_PASSWORD} # From .env\n",
        "    volumes:\n",
        "      - gitea_data:/data # Persistent volume for Gitea's internal data and Git repos\n",
        "    ports:\n",
        "      - \"3000:3000\" # Web UI\n",
        "      - \"2222:2222\" # SSH for Git (optional, if you use SSH cloning)\n",
        "    networks:\n",
        "      - ai_coder_net\n",
        "    restart: always\n",
        "    healthcheck:\n",
        "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000/\"]\n",
        "      interval: 10s\n",
        "      timeout: 5s\n",
        "      retries: 5\n",
        "\n",
        "  # 2. ArangoDB - Your Knowledge Graph Database\n",
        "  arangodb:\n",
        "    image: arangodb/arangodb:latest\n",
        "    container_name: arangodb-instance\n",
        "    # Use a fixed password from .env for easier integration with the ingestion pipeline\n",
        "    environment:\n",
        "      ARANGO_ROOT_PASSWORD: \"${ARANGO_ROOT_PASSWORD}\"\n",
        "    volumes:\n",
        "      - arangodb_data:/var/lib/arangodb3 # Persistent data volume\n",
        "    ports:\n",
        "      - \"8529:8529\" # ArangoDB Web UI and API\n",
        "    networks:\n",
        "      - ai_coder_net\n",
        "    restart: always\n",
        "    healthcheck:\n",
        "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8529/_admin/server/version\"]\n",
        "      interval: 10s\n",
        "      timeout: 5s\n",
        "      retries: 5\n",
        "\n",
        "  # 3. Ingestion Pipeline - Processes Git data into ArangoDB\n",
        "  ingestion_pipeline:\n",
        "    build: ./ingestion_pipeline # Build from Dockerfile in this directory\n",
        "    container_name: git-arangodb-ingest\n",
        "    depends_on:\n",
        "      arangodb:\n",
        "        condition: service_healthy # Ensure ArangoDB is ready\n",
        "    environment:\n",
        "      # ArangoDB Connection\n",
        "      ARANGO_HOST: arangodb-instance # Service name in Docker Compose network\n",
        "      ARANGO_PORT: 8529\n",
        "      ARANGO_USER: root\n",
        "      ARANGO_PASSWORD: \"${ARANGO_ROOT_PASSWORD}\" # Use the fixed password from .env\n",
        "\n",
        "      # Gitea Connection (for cloning repos)\n",
        "      GITEA_HOST: gitea-server # Service name in Docker Compose network\n",
        "      GITEA_HTTP_PORT: 3000\n",
        "      GITEA_ADMIN_USER: ${GITEA_ADMIN_USER}\n",
        "      GITEA_ADMIN_PASSWORD: ${GITEA_ADMIN_PASSWORD}\n",
        "      # This example assumes you'll clone via HTTP with admin credentials.\n",
        "      # For production, consider using a dedicated Git user token or SSH keys.\n",
        "\n",
        "      # Git Repo to Ingest (example, you'll update this)\n",
        "      GIT_REPO_ORG: \"ai-coder-org\" # Example organization on Gitea\n",
        "      GIT_REPO_NAME: \"sample-codebase\" # Example repository name on Gitea\n",
        "      # The full URL will be constructed in the Python script\n",
        "    volumes:\n",
        "      - ./repos:/app/repos # Mount a volume for cloning Git repos\n",
        "    networks:\n",
        "      - ai_coder_net\n",
        "    # command: python /app/main.py # Uncomment if you want to explicitly define the entrypoint\n",
        "    # entrypoint: [\"python\", \"/app/main.py\"] # Alternative to CMD in Dockerfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8bea017"
      },
      "source": [
        "# ingestion_pipeline/Dockerfile\n",
        "FROM python:3.10-slim-bookworm\n",
        "\n",
        "# Install Git and any other system dependencies needed for parsing/embeddings\n",
        "RUN apt-get update && \\\n",
        "    apt-get install -y git libmagic1 && \\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy requirements file first to leverage Docker cache\n",
        "COPY ingestion_pipeline/requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the application code\n",
        "COPY ingestion_pipeline/app/ .\n",
        "\n",
        "# Define a default command to run when the container starts\n",
        "CMD [\"python\", \"main.py\"]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}